{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNaFfaXaozw9",
        "outputId": "fcbeb3c1-8f89-45aa-a6dc-18e38f6c9d91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Unzipping /content/drive/MyDrive/augmented_pepper_dataset.zip...\n",
            "Unzip complete!\n",
            "\n",
            "--- Dataset Structure ---\n",
            "pepper_inspection/\n",
            "    content/\n",
            "        dataset/\n",
            "            UNHEALTHY/\n",
            "                Slow-Decline/\n",
            "                    IMG_20241226_171955.jpeg\n",
            "                    IMG_20250315_123042.jpeg.jpeg.jpeg.jpeg\n",
            "                    ... (500 total files)\n",
            "                augmented/\n",
            "                    IMG_20250315_113118.jpeg__crop.jpeg\n",
            "                    PYMV059__poisson_noise.jpg\n",
            "                    ... (32736 total files)\n",
            "                black_pepper_yellow_mottle_virus/\n",
            "                    PYMV080.jpg\n",
            "                    PYMV211.jpg\n",
            "                    ... (273 total files)\n",
            "                Pollu_Disease/\n",
            "                    IMG_20250311_160634.jpeg\n",
            "                    IMG_20250131_161214.jpeg\n",
            "                    ... (500 total files)\n",
            "                MERGED/\n",
            "                    PYMV080.jpg\n",
            "                    IMG_20250311_160634.jpeg\n",
            "                    ... (2046 total files)\n",
            "                black_pepper_leaf_blight/\n",
            "                    LB0024.jpg\n",
            "                    LB0261.jpg\n",
            "                    ... (273 total files)\n",
            "                Footrot/\n",
            "                    1739706136948.jpeg\n",
            "                    1739706136904.jpeg\n",
            "                    ... (500 total files)\n",
            "            HEALTHY/\n",
            "                augmented/\n",
            "                    image-209-_jpg.rf.86e71dce79611459b6574125a9a549ba__translated.jpg\n",
            "                    PH0054__hist_equalized.png\n",
            "                    ... (15216 total files)\n",
            "                Black pepper detection1.v1i.yolov8/\n",
            "                    data.yaml\n",
            "                    README.roboflow.txt\n",
            "                    ... (3 total files)\n",
            "                    train/\n",
            "                        images/\n",
            "                            image-155-_jpg.rf.3b98201b7c3df60b2b3cb532d20259fa.jpg\n",
            "                            image-2-_png_jpg.rf.f19632de812b2ebbfedcef47e7cd602e.jpg\n",
            "                            ... (620 total files)\n",
            "                        labels/\n",
            "                            image-167-_jpg.rf.933ad4262fe084ff0d976c7124fb2d2b.txt\n",
            "                            image-10-_jpg.rf.c1aa387a9f6a9a33f5361e00d5331531.txt\n",
            "                            ... (620 total files)\n",
            "                    test/\n",
            "                        images/\n",
            "                            image-66-_jpg.rf.a02cfbf9b2c2248b7d6c438bef41724b.jpg\n",
            "                            image-105-_jpg.rf.5aa615ba898651546f803c4c6ebe3ee7.jpg\n",
            "                            ... (29 total files)\n",
            "                        labels/\n",
            "                            image-90-_jpg.rf.18f06ebca3e8bbe74b08ce9240f56ee8.txt\n",
            "                            image-249-_jpg.rf.97793de363001d1431381622a3d0053a.txt\n",
            "                            ... (29 total files)\n",
            "                    valid/\n",
            "                        images/\n",
            "                            image-243-_jpg.rf.520533d72762108afbc51c74ca80d1e1.jpg\n",
            "                            image-342-_jpg.rf.4677387821c49a3cfc709284662dd312.jpg\n",
            "                            ... (58 total files)\n",
            "                        labels/\n",
            "                            image-140-_jpg.rf.112ad4791eaff651a1dd9a2d5a687133.txt\n",
            "                            image-342-_jpg.rf.4677387821c49a3cfc709284662dd312.txt\n",
            "                            ... (58 total files)\n",
            "                MERGED/\n",
            "                    PH0168.png\n",
            "                    image-155-_jpg.rf.3b98201b7c3df60b2b3cb532d20259fa.jpg\n",
            "                    ... (951 total files)\n",
            "                black pepper RAW/\n",
            "                    black_pepper_792.jpg\n",
            "                    black_pepper_456.JPG\n",
            "                    ... (1000 total files)\n",
            "                black_pepper_healthy/\n",
            "                    PH0168.png\n",
            "                    PH0178.png\n",
            "                    ... (273 total files)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Define Paths\n",
        "zip_path = '/content/drive/MyDrive/augmented_pepper_dataset.zip'\n",
        "extract_path = '/content/pepper_inspection'\n",
        "\n",
        "# 3. Unzip the file\n",
        "print(f\"Unzipping {zip_path}...\")\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "print(\"Unzip complete!\\n\")\n",
        "\n",
        "# 4. Check what's inside (Recursive Directory View)\n",
        "print(\"--- Dataset Structure ---\")\n",
        "for root, dirs, files in os.walk(extract_path):\n",
        "    # This calculates the depth to make the print look like a tree\n",
        "    level = root.replace(extract_path, '').count(os.sep)\n",
        "    indent = ' ' * 4 * (level)\n",
        "    print(f\"{indent}{os.path.basename(root)}/\")\n",
        "\n",
        "    # Optional: Print first 2 files in each folder to verify content\n",
        "    sub_indent = ' ' * 4 * (level + 1)\n",
        "    for f in files[:2]:\n",
        "        print(f\"{sub_indent}{f}\")\n",
        "    if len(files) > 2:\n",
        "        print(f\"{sub_indent}... ({len(files)} total files)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvyyoOysrvHs",
        "outputId": "1e381acb-52ed-4cd7-ad44-53c2c9ae7d5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Using device: cuda\n",
            "Downloading: \"https://download.pytorch.org/models/squeezenet1_1-b8a52dc0.pth\" to /root/.cache/torch/hub/checkpoints/squeezenet1_1-b8a52dc0.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.73M/4.73M [00:00<00:00, 192MB/s]\n",
            "Epoch 1/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2398/2398 [07:13<00:00,  5.53batch/s, acc=1.0000, loss=0.0324]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1 Validation Accuracy: 0.9970\n",
            "ðŸŒŸ Best model saved to Drive! (Acc: 0.9970)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2398/2398 [07:09<00:00,  5.58batch/s, acc=1.0000, loss=0.0001]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2 Validation Accuracy: 0.9980\n",
            "ðŸŒŸ Best model saved to Drive! (Acc: 0.9980)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2398/2398 [07:03<00:00,  5.66batch/s, acc=1.0000, loss=0.0002]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3 Validation Accuracy: 0.9638\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2398/2398 [06:54<00:00,  5.78batch/s, acc=1.0000, loss=0.0001]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4 Validation Accuracy: 0.9993\n",
            "ðŸŒŸ Best model saved to Drive! (Acc: 0.9993)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2398/2398 [06:53<00:00,  5.80batch/s, acc=1.0000, loss=0.0000]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 5 Validation Accuracy: 0.9948\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2398/2398 [06:59<00:00,  5.72batch/s, acc=1.0000, loss=0.0002]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 6 Validation Accuracy: 0.9990\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2398/2398 [06:57<00:00,  5.74batch/s, acc=1.0000, loss=0.0000]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 7 Validation Accuracy: 0.9995\n",
            "ðŸŒŸ Best model saved to Drive! (Acc: 0.9995)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2398/2398 [06:59<00:00,  5.72batch/s, acc=1.0000, loss=0.0000]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 8 Validation Accuracy: 0.9998\n",
            "ðŸŒŸ Best model saved to Drive! (Acc: 0.9998)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2398/2398 [06:54<00:00,  5.78batch/s, acc=1.0000, loss=0.0003]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 9 Validation Accuracy: 0.9997\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2398/2398 [06:59<00:00,  5.72batch/s, acc=1.0000, loss=0.0000]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 10 Validation Accuracy: 0.9998\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from PIL import Image\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Mount Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Path Configuration\n",
        "# These are based on your specific unzipped directory structure\n",
        "healthy_path = '/content/pepper_inspection/content/dataset/HEALTHY/augmented'\n",
        "unhealthy_path = '/content/pepper_inspection/content/dataset/UNHEALTHY/augmented'\n",
        "save_path = '/content/drive/MyDrive/best_pepper_squeezenet.pth'\n",
        "\n",
        "# 3. Custom Dataset Class to handle the massive file count\n",
        "class PepperDataset(Dataset):\n",
        "    def __init__(self, healthy_dir, unhealthy_dir, transform=None):\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "\n",
        "        # Load Healthy images (Label 0)\n",
        "        for f in os.listdir(healthy_dir):\n",
        "            if f.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                self.images.append((os.path.join(healthy_dir, f), 0))\n",
        "\n",
        "        # Load Unhealthy images (Label 1)\n",
        "        for f in os.listdir(unhealthy_dir):\n",
        "            if f.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                self.images.append((os.path.join(unhealthy_dir, f), 1))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.images[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# 4. Transforms & Loading\n",
        "# Keeping your 500x500 resolution from ResNet18\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize((500, 500)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "full_dataset = PepperDataset(healthy_path, unhealthy_path, transform=data_transforms)\n",
        "\n",
        "# Split 80/20\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_set, val_set = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# SqueezeNet is lightweight, so we can use a slightly higher batch size than 8\n",
        "train_loader = DataLoader(train_set, batch_size=16, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_set, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "# 5. Model Initialization (SqueezeNet PLC-SN)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = models.squeezenet1_1(weights='IMAGENET1K_V1')\n",
        "# Adjusting classifier for 2 classes (Healthy vs Unhealthy)\n",
        "model.classifier[1] = nn.Conv2d(512, 2, kernel_size=(1,1))\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.00005)\n",
        "\n",
        "# 6. Training Function\n",
        "def train_model(epochs=10):\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        # Training Progress Bar\n",
        "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}', unit='batch')\n",
        "\n",
        "        for inputs, labels in pbar:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            # Show live batch accuracy\n",
        "            pbar.set_postfix(loss=f\"{loss.item():.4f}\", acc=f\"{torch.sum(preds == labels.data).item()/inputs.size(0):.4f}\")\n",
        "\n",
        "        # Validation Phase\n",
        "        model.eval()\n",
        "        val_corrects = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                val_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        epoch_acc = val_corrects.double() / val_size\n",
        "        print(f\"\\nEpoch {epoch+1} Validation Accuracy: {epoch_acc:.4f}\")\n",
        "\n",
        "        # Save Best Weights to Drive\n",
        "        if epoch_acc > best_acc:\n",
        "            best_acc = epoch_acc\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(f\"ðŸŒŸ Best model saved to Drive! (Acc: {best_acc:.4f})\")\n",
        "\n",
        "# 7. Start Training\n",
        "train_model(epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 614
        },
        "id": "-WLQ2g-XB3XQ",
        "outputId": "f11e9cd9-87f5-46e3-fa08-c680413b6be8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://ce9ab834eae14be5d3.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://ce9ab834eae14be5d3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install -q gradio\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "import gradio as gr\n",
        "from PIL import Image\n",
        "\n",
        "# 1. Load the Model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def load_model():\n",
        "    model = models.squeezenet1_1()\n",
        "    model.classifier[1] = nn.Conv2d(512, 2, kernel_size=(1,1))\n",
        "\n",
        "    # Update this path if your filename is different\n",
        "    model_path = '/content/drive/MyDrive/best_pepper_squeezenet.pth'\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "pepper_model = load_model()\n",
        "\n",
        "# 2. Prediction Function for Gradio\n",
        "def predict_image(inp_img):\n",
        "    if inp_img is None:\n",
        "        return None\n",
        "\n",
        "    # Preprocessing (matching your 500x500 training)\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.Resize((500, 500)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Prepare image\n",
        "    input_tensor = preprocess(inp_img).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = pepper_model(input_tensor)\n",
        "        # SqueezeNet outputs raw logits, so we apply softmax for percentages\n",
        "        probs = torch.nn.functional.softmax(outputs[0], dim=0)\n",
        "\n",
        "    # Mapping results\n",
        "    labels = ['HEALTHY', 'UNHEALTHY']\n",
        "    return {labels[i]: float(probs[i]) for i in range(2)}\n",
        "\n",
        "# 3. Create Gradio Interface\n",
        "interface = gr.Interface(\n",
        "    fn=predict_image,\n",
        "    inputs=gr.Image(type=\"pil\"),\n",
        "    outputs=gr.Label(num_top_classes=2),\n",
        "    title=\"PepperAI: Fungal Detection System (SqueezeNet)\",\n",
        "    description=\"Upload a photo of a pepper leaf or fruit to determine if it is Healthy or Unhealthy. Powered by PLC-SN architecture.\",\n",
        "    theme=\"soft\"\n",
        ")\n",
        "\n",
        "interface.launch(share=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
