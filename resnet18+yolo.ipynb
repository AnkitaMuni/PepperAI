{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48011dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c99860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://3d4f64437274c2a69c.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://3d4f64437274c2a69c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x416 (no detections), 385.5ms\n",
      "Speed: 31.0ms preprocess, 385.5ms inference, 25.3ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 512x640 1 banana, 207.5ms\n",
      "Speed: 4.5ms preprocess, 207.5ms inference, 24.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 1 banana, 280.5ms\n",
      "Speed: 6.0ms preprocess, 280.5ms inference, 1.9ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 448x640 (no detections), 158.3ms\n",
      "Speed: 5.2ms preprocess, 158.3ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 (no detections), 152.5ms\n",
      "Speed: 4.7ms preprocess, 152.5ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Created dataset file at: .gradio/flagged/dataset1.csv\n",
      "\n",
      "0: 640x480 (no detections), 185.4ms\n",
      "Speed: 6.0ms preprocess, 185.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 (no detections), 163.8ms\n",
      "Speed: 5.7ms preprocess, 163.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x640 1 umbrella, 347.3ms\n",
      "Speed: 14.3ms preprocess, 347.3ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gradio as gr\n",
    "from ultralytics import YOLO\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- STEP 1: LOAD YOUR TRAINED CLASSIFIER ---\n",
    "def load_pepper_model(model_path):\n",
    "    model = models.resnet18(weights=None)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 2)\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model not found at: {model_path}\")\n",
    "        \n",
    "    state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# --- STEP 2: SETUP ---\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# UPDATE THIS PATH to your actual .pth file location\n",
    "MODEL_PATH = '/content/drive/MyDrive/pepper_resnet18_final.pth'\n",
    "\n",
    "detector = YOLO('yolov8n.pt') \n",
    "classifier = load_pepper_model(MODEL_PATH)\n",
    "class_names = ['Healthy', 'Unhealthy'] \n",
    "\n",
    "# --- STEP 3: LOGIC FOR GRADIO ---\n",
    "def predict_pepper(input_img):\n",
    "    # Gradio provides images as numpy arrays by default\n",
    "    img = np.array(input_img)\n",
    "    img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    img_display = img_bgr.copy()\n",
    "    \n",
    "    results = detector(img)\n",
    "    detections_found = False\n",
    "    \n",
    "    for result in results:\n",
    "        if len(result.boxes) > 0:\n",
    "            detections_found = True\n",
    "            for box in result.boxes:\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                process_and_draw(img_display, img, x1, y1, x2, y2)\n",
    "    \n",
    "    if not detections_found:\n",
    "        h, w, _ = img.shape\n",
    "        process_and_draw(img_display, img, 0, 0, w, h)\n",
    "\n",
    "    # Return as RGB for Gradio to display correctly\n",
    "    return cv2.cvtColor(img_display, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def process_and_draw(img_display, img_rgb, x1, y1, x2, y2):\n",
    "    crop = img_rgb[y1:y2, x1:x2]\n",
    "    if crop.size == 0: return\n",
    "    \n",
    "    pil_img = Image.fromarray(crop)\n",
    "    input_tensor = preprocess(pil_img).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = classifier(input_tensor)\n",
    "        index = torch.argmax(output, dim=1).item()\n",
    "        label = class_names[index]\n",
    "    \n",
    "    color = (0, 255, 0) if label == 'Healthy' else (0, 0, 255) # BGR\n",
    "    cv2.rectangle(img_display, (x1, y1), (x2, y2), color, 5)\n",
    "    cv2.putText(img_display, label, (x1 + 5, y1 + 35), cv2.FONT_HERSHEY_SIMPLEX, 1.2, color, 3)\n",
    "\n",
    "# --- STEP 4: LAUNCH GRADIO ---\n",
    "demo = gr.Interface(\n",
    "    fn=predict_pepper,\n",
    "    inputs=gr.Image(label=\"Drag & Drop Pepper Image here\"),\n",
    "    outputs=gr.Image(label=\"Analysis Result\"),\n",
    "    title=\"üå∂Ô∏è PepperAI: Health Scanner\",\n",
    "    description=\"Upload an image of a pepper plant to detect health issues.\"\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
